---
title: "Chollet's ideas on LLMs as vector program databases"
date: 2023-10-19
permalink: /posts/2023/10/chollet-llm/
tags:
  - posts
---

A fun [substack](https://fchollet.substack.com/p/how-i-think-about-llm-prompt-engineering) article from Francois Chollet, the creator of Keras. He describes a fascinating way to think about LLM prompts as indexes into
continuous vector space of programs. The analogy he gives is of word2vec, the same way word2vec had an emergent property of 'word arithmetic', for example, there existed a 'gender vector' that worked as such,
$V_{king} - V_{man} + V_{woman} = V_{queen}$ . There were many more, like one that went from a word to its plural. 

Application to LLMs
Chollet argues that just like word2vec encoded a gender vector, modern LLMs with their billions of parameters encode much more complex vector functions, that translate one set of embeddings to another. For example if
word2vec encoded a function like `male_to_female(king) -> queen`, LLMs have something like `rewrite_in_the_style_of_shakespeare(my_input) -> generated_output`. These functions are so complex that
Chollet argues that they should in fact be treated as vector programs. So we can think of prompts as indexes into the LLM's data store that has many, many such complicated programs.
